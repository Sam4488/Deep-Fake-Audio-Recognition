

## ğŸ”Š Deep-Fake Audio Recognition

### ğŸ“ Project Overview
This project focuses on detecting **deepfake audio**â€”synthetic voice recordings created using advanced AI techniques like speech synthesis, voice conversion, and replay attacks. With the growing threat of voice manipulation in sensitive fields like **banking, customer service, and law enforcement**, itâ€™s crucial to develop robust methods to differentiate real voices from fake ones.

Our approach combines powerful **deep learning models** with smart **feature extraction** and **transfer learning** techniques to spot fake audio with high accuracy.

---

### ğŸ§  Models Used
To tackle deepfake audio detection, we experimented with a diverse set of powerful modelsâ€”each bringing unique strengths to the table:

ğŸ” **BiLSTM (Bidirectional LSTM)**
Captures temporal patterns in audio sequences from both past and future directions for better context understanding.

ğŸ”Š **SE-Enhanced 1D-CNN for Time Series Classification**
Combines Squeeze-and-Excitation blocks with 1D CNNs to boost important audio features while suppressing noise.

ğŸ§± **Custom CNN Architecture**
Tailor-made convolutional layers designed to extract deep, hierarchical features from spectrograms and MFCCs.

ğŸŒ² **Random Forest Classifier**
A robust ensemble-based method used for baseline comparisons and feature importance analysis.

ğŸŒ **WIREnet**
A deep learning model tailored for audio spoofing detection, known for its effectiveness on ASVspoof datasets.

ğŸŒ€ **ResNet FC (Fully Convolutional Residual Network)**
Utilizes residual connections for deep feature learning, reducing vanishing gradient issues.

ğŸ¯ **ResCNN-Attention-BiGRU**
A hybrid model that merges CNNs with Attention-powered Bidirectional GRUs for both spatial and sequential feature focus.

ğŸ§® **SVM (Support Vector Machine)**
Classic yet effective, SVMs are used here for linear and non-linear classification on extracted feature vectors.

---

### ğŸµ Audio Features Extracted
To detect fakes effectively, we extract the following features:
- **Spectrograms**
- **MFCCs** (Mel-Frequency Cepstral Coefficients)
- **Chroma STFT**
- **Spectral Centroid**
- **Spectral Bandwidth**
- **Zero Crossing Rate**

All features are **normalized** to maintain consistency across samples.

---

### ğŸ“Š Dataset Used

#### ğŸ“Œ ASVspoof 2019 (Main Dataset)
A benchmark dataset for automatic speaker verification spoofing detection.

- **Logical Access (LA)**: Includes synthetic voices via TTS and voice conversion.
- **Physical Access (PA)**: Includes replayed voices in real-world scenarios.

**Data Split**:
- Training: 17,615 samples  
- Validation: 5,413 samples  
- Testing: 3,212 samples  

#### ğŸ“ Additional Datasets:
- **RFP Dataset**
- **FakeAVCeleb Dataset**

---

### âš™ï¸ Methodology

#### 1. **Data Preprocessing**
- Clean audio
- Extract relevant features
- Normalize the data

#### 2. **Model Training**
- Models built using **PyTorch**
- Trained on the extracted features
- Evaluation via **cross-validation**

#### 3. **Evaluation Metrics**
- **F1-Score**
- **Tandem Decision Cost Function (t-DCF)**

---

### ğŸ† Results
Our models showed **high accuracy on the ASVspoof 2019 dataset**, proving their ability to detect deepfake audio in controlled environments. However, challenges remain when generalizing to external datasetsâ€”a common issue in real-world deepfake detection.

---

### ğŸš€ Future Enhancements

- **Domain Adaptation**: Boost generalization using noise injection, pitch shifting, and speed variation.
- **Multimodal Learning**: Integrate **audio with video or text** for better accuracy.
- **Self-Supervised Learning**: Use **Wav2Vec 2.0** and **HuBERT** for stronger feature learning.
- **Graph Neural Networks**: Capture relationships between audio features more effectively.
- **Quantum Computing**: Explore faster computation for deepfake detection models.

---

### ğŸ‘¨â€ğŸ’» Contributors
- Kavya Verma  
- Ojas Kulkarni  
- Sagnik Samanta  
- **Kabir Gulati**  
- Divyanshi Mittal  
- Muzaffar Ahmad Dar  
- C. L. Biji  

---
